<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LIS 500 Final Project | Cade</title>
    <link rel="icon" href="images/Wisconsin_Badgers_logo.svg.png">
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Dancing+Script&family=Open+Sans:wght@300;400&display=swap');
    </style>
    <link rel="stylesheet" href="index.css">
</head>

<!--header of the home page of the website-->

<body>
    <header>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="survey.html">Take Survey</a></li>
                <li><a href="results.html">Results</a></li>
                <li><a href="analysis.html">Analysis</a></li>
                <li><a href="resources.html">Resources</a></li>
            </ul>
        </nav>
    </header>
    <!--container for home page content, used for formatt-->
    <div class="home-content">
        <!--title of the home page content-->

        <section class="home-content-blurb">
            <h2 class="home-content-blurb-title">Cade's Essay</h2>
            <article>
                <p>Hi everyone! I am a senior at UW Madison studying Computer Science. I am particulary interested in
                    web development and hope to one day make it my full time career. Going into this course I had quite
                    a bit of experience with web development, but I was not as familiar with the social constructs in
                    the tech industry. However, this course along with the many group discussions, readings, and
                    assignments has given me the tools to develop a more socially aware perspective of the tech
                    industry.
                </p>

                <p>When choosing a topic for this project, my mind went back to the video from Discussion 3, “How Ellen
                    Pao realized women ‘cannot succeed’ in Silicon Valley frat boy culture”. In this video, Ellen Pao
                    discusses how her and her female coworkers are under appreciated in the workplace, despite
                    performing equally well if not better in the workplace. Due to her coworkers implicit biases about
                    women, they do not treat her as an equal. Ellen Pao says that women go unrepresented for their hard
                    work and contributions in the work place due to the fact that they are women and do not play along
                    with the frat boy culture of the men that is often a degrading community of sexism, racism, and
                    other discrimination.
                </p>
                <p>In Algorithms of Oppression, there were many cases of implicit bias being in the workplace, in
                    public, and most notably on search engines. Artificial intelligence tools are learning
                    discrimination and it is becoming a serious problem for tech companies worldwide. One notorious case
                    was that of Amazon’s recruitment software built-in 2014. The tool was designed to rate applicants'
                    resumes on a scale of one to five, based on how good of a candidate they were. However, by 2015 the
                    company realized that the software was rating candidates in a non gender neutral manner. In fact,
                    resumes containing the word “women'' such as “women’s chess club captain” were being penalized. The
                    mechanism for this was due to Amazon’s recruiting tool being calibrated using resumes submitted to
                    the company over the last 10 years. Which ultimately, was a reflection of the largely male dominated
                    tech industry. This caused the recruiting tool to prefer resumes from male applicants, as it was fed
                    data telling it to do so.[1] </p>
                <p>Another similar case of sexism among AI was an earlier version of
                    Google Translate. When users would type a sentence such as “she is a surgeon” and translate it to a
                    language with no gendered pronouns, then translate it back they would get “he is a surgeon” instead.
                    This was not just a single anomaly either as users were noticing that male pronouns were being
                    assigned to translations time and time again, most notably if the subject was relating to
                    traditionally more male dominated fields like science and engineering. Although Google has “fixed”
                    this issue by putting a system in place to now give users both the female and male pronouns, it
                    makes users wonder what else Google algorithms are biased toward that we have yet to uncover.[2]
                </p>
                <p>However, these biases are not due to an error in the algorithms that the AI runs on. They are
                    derived from the datasets of web pages, books, pictures, and videos that they are learning from. In
                    a society where systemic racism, sexism, ableism, homophobia, and more inequality are deeply
                    embedded, it is difficult to acquire a dataset that is representative of equality. Companies can try
                    to avoid this by setting preventative rules in place but there is no guaranteed way to test if their
                    AI is being completely unbiased. Even so, there remains the problem of AI systems that are trained
                    mostly based on a single user's engagement. Such as the algorithms used by Tik Tok to recommend
                    users new videos on their For You Page. A team of researchers tested their algorithms by creating
                    new Tik Tok accounts and choosing to interact solely with transphobic content, as that is often seen
                    to be a gateway prejudice into more hateful and far-right content. After doing so, the team analyzed
                    the subsequent 400 recommended videos and found that they were largely populated by an even wider
                    range of radical content. Topics such as homophobia, misogyny, and white supremacy were amongst the
                    top number of recommended videos.[3] This finding is alarming as Tik Tok is the world’s most
                    downloaded app with over 2.6 billion downloads by 2020, many of which are children.[4] This sort of
                    prejudice pipeline is not just Tik Tok specific either, as similar trends have been seen amongst
                    Google and other search engines that use algorithms to display top results for users. This in turn
                    propagates the original problem of AI becoming discriminatory, as it is learning from these top
                    results on the internet. At the end of the day, a machine learning algorithm will only be as good as
                    the datasets it feeds on. Which happens to be a society built upon implicit biases.
                </p>
                <p>For this reason, I thought it would be interesting to capture the implicit bias of our peers towards
                    gender in the workplace and reflect on whether or not taking this course had an impact on my own
                    biases. I think working in a remote programming environment almost made it a little harder to
                    coordinate how much work each group member should do. We did not want to follow the same mistakes of
                    big tech companies and place the tasks such as scheduling meetings, turning in assignments, or other
                    menial tasks on the female group members like Ellen Pao discussed in her interview. Overall the
                    process of doing this project and the results we gathered have shaped my perspective on the tech
                    industry and I hope it gave insight into a chunk of students here at UW Madison.
                </p>
                <p>[1] Dastin, Jeffrey. “Amazon Scraps Secret AI Recruiting Tool That Showed Bias against Women.”
                    Reuters. Thomson Reuters, October 10, 2018.
                    https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G.
                </p>
                <p>[2] Quach, Katyanna. “US Homeland Security Installs AI Cameras at the White House, Google Tries to
                    Make Translation Less Sexist.” The Register® - Biting the hand that feeds IT. The Register, December
                    9, 2018. https://www.theregister.com/2018/12/08/ai_roundup_081218/.
                </p>
                <p>[3] Written by Olivia Little & Abbie Richards, and Carly Evans & Jeremy Tuthill Research
                    contributions from Nena Beecham. “TikTok's Algorithm Leads Users from Transphobic Videos to
                    Far-Right Rabbit Holes.” Media Matters for America, October 5, 2021.
                    https://www.mediamatters.org/tiktok/tiktoks-algorithm-leads-users-transphobic-videos-far-right-rabbit-holes.
                </p>
                <p>[4] “Tiktok Statistics - Everything You Need to Know [Aug 2022 Update].” Wallaroo Media, August 13,
                    2022.
                    https://wallaroomedia.com/blog/social-media/tiktok-statistics/#:~:text=Total%20App%20Downloads%20%E2%80%93%20The%20TikTok,quarter%20by%20any%20app%2C%20ever.
                </p>
            </article>
        </section>
    </div>
    <!--footer of the HTML page-->
    <footer>
        <h3>Website Info:</h3>
        <p>Survey Created by Aaron, Marley, Cade, and Beda</p>
        <p>LIS 500 UW Madison</p>
    </footer>

</body>

</html>